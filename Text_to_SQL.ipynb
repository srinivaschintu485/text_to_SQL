{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL3byIHfWEVx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "The command !pip install unsloth trl peft accelerate bitsandbytes installs a suite of Python libraries commonly used for fine-tuning and deploying large language models efficiently. unsloth provides fast and memory-efficient tools for fine-tuning models like Mistral and Phi using LoRA (Low-Rank Adaptation) with optimized performance. trl (Transformers Reinforcement Learning) includes tools like SFTTrainer for supervised fine-tuning of LLMs. peft (Parameter-Efficient Fine-Tuning) enables LoRA and other lightweight fine-tuning methods. accelerate simplifies multi-GPU and mixed-precision training setups. Finally, bitsandbytes allows models to be quantized to 8-bit or 4-bit, reducing memory usage significantly and enabling model training or inference on consumer GPUs like T4 or 3060. This combined setup is ideal for training powerful LLMs even on limited hardware."
      ],
      "metadata": {
        "id": "GXnMk8i1MZcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes #"
      ],
      "metadata": {
        "id": "dvzg20lCMYvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "alpaca_prompt\n",
        "\n",
        "The alpaca_prompt is a multi-line string template that is used for training a model to generate SQL queries.\n",
        "\n",
        "Instruction Section:\n",
        "\n",
        "### Instruction:\n",
        "\n",
        "This section is where you describe the task or the SQL query in a human-readable way. For example, the database context or the SQL task you want to generate.\n",
        "\n",
        "{company_database}: A placeholder for the actual database schema or description (such as the tables and relationships involved in the SQL query).\n",
        "\n",
        "Input Section:\n",
        "\n",
        "### Input:\n",
        "\n",
        "This is where the SQL prompt/question is given as input.\n",
        "\n",
        "{prompt}: A placeholder for the SQL query prompt that you want the model to interpret and generate SQL for. This is typically a user query like \"What is the total sales in the North region?\"\n",
        "\n",
        "Response Section:\n",
        "\n",
        "### Response:\n",
        "\n",
        "This is the expected output section, where the model is supposed to generate a response.\n",
        "\n",
        "{sql}: The placeholder for the actual SQL query that should be generated by the model based on the prompt. For example, SELECT SUM(sales) FROM sales_table WHERE region = 'North'.\n",
        "\n",
        "{explanation}: This is the explanation of how the SQL query was derived or why it solves the problem in the prompt.\n",
        "\n",
        "<|endoftext|>:\n",
        "\n",
        "This is a special token that signifies the end of the text in certain prompt designs. It helps in tokenizing and processing input and output efficiently, especially in sequence-based tasks like text generation."
      ],
      "metadata": {
        "id": "d8dJFwwgPNsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salesforce/codegen-350M-mono\n",
        "Salesforce/codegen-350M-mono is a specific model identifier within the CodeGen family developed by Salesforce.\n",
        "\n",
        "350M: Refers to the model's size, meaning it has 350 million parameters. This is a mid-sized model in terms of the number of parameters, providing a balance between computational efficiency and performance.\n",
        "\n",
        "mono: This refers to the monolingual version of the model. It is trained on a single language, which means it focuses on understanding and generating code in one specific language. The \"mono\" variant contrasts with a multilingual model that would support multiple languages.\n",
        "\n",
        " Model Purpose\n",
        "Code Generation: The codegen-350M-mono model is fine-tuned for generating code, making it highly suitable for tasks such as text-to-SQL generation, code completion, code translation, and other coding-related applications.\n",
        "\n",
        "The model can take in a prompt (natural language or some code) and generate code that solves a problem or completes a task based on the input."
      ],
      "metadata": {
        "id": "dB97ppNITDeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important parameters for the tokenizer include:**\n",
        "\n",
        "padding: Controls how the model handles sequences of varying lengths (i.e., sequences of text longer than the model's maximum sequence length).\n",
        "\n",
        "truncation: This parameter ensures that text longer than the model's input length is truncated to fit within the maximum input length.\n",
        "\n",
        "max_length: This specifies the maximum number of tokens allowed for the input sequence. For models like Salesforce/codegen-350M-mono, you would typically choose a max_length that corresponds to the model's capacity.\n",
        "\n",
        "return_tensors: Defines the format of the output (e.g., pt for PyTorch tensors). This is required when passing the input to the model for inference.\n",
        "\n",
        "add_special_tokens: Ensures that special tokens (like [CLS], [SEP], etc.) are added during tokenization.\n",
        "\n",
        "**Model Parameters**\n",
        "max_length: Refers to the maximum sequence length of tokens the model can process. For example, a model may have a max_length of 512 tokens, meaning any input sequence longer than 512 tokens would need to be truncated or split.\n",
        "\n",
        "temperature: Affects the randomness of the model's output. Higher temperatures (e.g., 0.8 or 1.0) make the model's output more random, while lower temperatures (e.g., 0.2 or 0.3) make the model's output more deterministic.\n",
        "\n",
        "top_p (nucleus sampling): Affects the diversity of the output by controlling the cumulative probability of the most likely next tokens. Only tokens within the top p cumulative probability are considered.\n",
        "\n",
        "top_k: Restricts the sampling pool to the top k most likely tokens, limiting the possible outputs at each step.\n",
        "\n",
        "do_sample: If set to True, the model generates text by sampling from the distribution of possible next tokens, adding randomness to the generated sequence.\n",
        "\n",
        "use_cache: If set to True, the model will cache the past attention state for faster inference.\n"
      ],
      "metadata": {
        "id": "UJhZifSOiVKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n"
      ],
      "metadata": {
        "id": "5r-VAcJYaX0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset                #This imports the Dataset class from Hugging Face's datasets library. It's used to convert data (like from a Pandas DataFrame or CSV) into a format that can be used directly with Hugging Face tools like transformers.\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import pandas as pd\n",
        "splits = {'train': 'synthetic_text_to_sql_train.snappy.parquet', 'test': 'synthetic_text_to_sql_test.snappy.parquet'}\n",
        "df = pd.read_parquet(\"hf://datasets/gretelai/synthetic_text_to_sql/\" + splits[\"train\"])\n",
        "df.isnull().sum()\n",
        "dataset = Dataset.from_pandas(df)\n",
        "#These are paths to two dataset files in Parquet format (compressed with Snappy), often used for efficient data storage and access.\n",
        "# Define Alpaca-style SQL prompt template\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input and expected output.\n",
        "### Instruction:\n",
        "Company database: {company_database}\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: {prompt}\n",
        "\n",
        "### Response:\n",
        "SQL: {sql}\n",
        "Explanation: {explanation}<|endoftext|>\"\"\"\n",
        "\n",
        "\n",
        "# Format each example into a single text string\n",
        "def format_prompt(example):\n",
        "    return alpaca_prompt.format(\n",
        "        company_database=example['sql_context'],\n",
        "        prompt=example['sql_prompt'],\n",
        "        sql=example['sql'],\n",
        "        explanation=example['sql_explanation']\n",
        "    )\n",
        "\n",
        "# Format your dataset from pandas dataframe `df`\n",
        "formatted_data = [format_prompt(row) for _, row in df.iterrows()]\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "\n",
        "# Ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Done! Now pass this dataset to SFTTrainer like this:\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     train_dataset=dataset,\n",
        "#     dataset_text_field=\"text\",\n",
        "#     ...\n",
        "# )\n",
        "\n",
        "# Optional: preview first row\n",
        "print(dataset[0]['text'])\n",
        "dataset = dataset.select(range(200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAJwKsIn--4x",
        "outputId": "a2893625-328e-40cf-97c7-65a59aee8e46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input and expected output.\n",
            "### Instruction:\n",
            "Company database: CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\n",
            "\n",
            "### Input:\n",
            "SQL Prompt: What is the total volume of timber sold by each salesperson, sorted by salesperson?\n",
            "\n",
            "### Response:\n",
            "SQL: SELECT salesperson_id, name, SUM(volume) as total_volume FROM timber_sales JOIN salesperson ON timber_sales.salesperson_id = salesperson.salesperson_id GROUP BY salesperson_id, name ORDER BY total_volume DESC;\n",
            "Explanation: Joins timber_sales and salesperson tables, groups sales by salesperson, calculates total volume sold by each salesperson, and orders the results by total volume in descending order.<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is this important?\n",
        "Padding Tokens: Padding ensures that all sequences in a batch are the same length, which is necessary for efficient processing in parallel on GPUs.\n",
        "\n",
        "eos_token as Padding: If the tokenizer doesn’t define a padding token, using the eos_token as padding works because it allows the model to treat padding as part of the sequence, rather than using a separate, dedicated token. This is especially useful for sequence generation tasks like code generation or text generation, where the eos_token marks the end of the sequence and could also serve as padding."
      ],
      "metadata": {
        "id": "zxO5cKupi06T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOTWElUCWk_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532aefd8-ae47-4139-f3d9-81f06a773002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# For GPU check\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x953lw83WxnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31b0bc7-7621-4b0c-ab6d-24eec52884db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-4282502808.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.6.12: Fast Mistral patching. Transformers: 4.53.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
        "\n",
        "max_seq_length = 2048  # Choose sequence length\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIdADxFWXToO"
      },
      "outputs": [],
      "source": [
        "# from datasets import Dataset\n",
        "\n",
        "# def format_prompt(example):\n",
        "#     return f\"### Input: {example['input']}\\n### Output: {json.dumps(example['output'])}<|endoftext|>\"\n",
        "\n",
        "# formatted_data = [format_prompt(item) for item in file]\n",
        "# dataset = Dataset.from_dict({\"text\": formatted_data})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters:\n",
        "r=64 (LoRA Rank):\n",
        "\n",
        "LoRA rank (r) refers to the size of the low-rank matrix used for adaptation. The higher the rank, the more capacity the model has for learning the low-rank transformations, but at the cost of more memory and computation. A rank of 64 suggests that the low-rank matrices will have 64-dimensional projections.\n",
        "\n",
        "target_modules:\n",
        "\n",
        "This specifies which parts of the model will be fine-tuned using the LoRA adapters. Each item refers to a specific attention or projection layer in the transformer architecture.\n",
        "\n",
        "\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\": These are the query, key, value, and output projections used in the attention layers of the transformer model.\n",
        "\n",
        "\"gate_proj\", \"up_proj\", \"down_proj\": These are other parts of the model (such as those used in feed-forward layers, gate functions, or other internal layers) that will be adapted.\n",
        "\n",
        "lora_alpha=128:\n",
        "\n",
        "The scaling factor for the LoRA layers. This helps scale the low-rank updates in the model. A typical value is 2x the rank (so lora_alpha = 2 * r).\n",
        "\n",
        "A larger value here means that the low-rank adaptation will have a higher influence on the model.\n",
        "\n",
        "lora_dropout=0:\n",
        "\n",
        "This refers to dropout used in the LoRA layers. Dropout helps prevent overfitting during training by randomly dropping some connections. Here, it's set to 0, meaning no dropout is used.\n",
        "\n",
        "bias=\"none\":\n",
        "\n",
        "Specifies that the LoRA adapters will not introduce bias terms. LoRA typically operates without bias for efficiency.\n",
        "\n",
        "use_gradient_checkpointing=\"unsloth\":\n",
        "\n",
        "Gradient checkpointing is used to reduce memory consumption during training by saving only certain layers of gradients and recomputing others. The \"unsloth\" here refers to a custom, optimized version provided by the Unsloth framework for better performance.\n",
        "\n",
        "random_state=3407:\n",
        "\n",
        "A fixed random state value to ensure reproducibility of the training process. This ensures that if you run the training multiple times, the results will be consistent.\n",
        "\n",
        "use_rslora=False:\n",
        "\n",
        "This parameter controls whether to use Rank Stabilized LoRA (rSLORA), a variant that stabilizes the rank of the low-rank approximations during fine-tuning. It's set to False, meaning the standard LoRA is used.\n",
        "\n",
        "loftq_config=None:\n",
        "\n",
        "LoftQ is a specific configuration that can be used with LoRA to further optimize training with quantized weights. Since it's set to None, it means LoftQ is not used in this case."
      ],
      "metadata": {
        "id": "wIzPV3WNONBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v08de3wAXdu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64610794-8480-4090-db77-5dee1d60d69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.6.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # Rank stabilized LoRA\n",
        "    loftq_config=None, # LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters and Their Explanation:\n",
        "model:\n",
        "\n",
        "The model that has been fine-tuned or is ready for fine-tuning. It's passed to the SFTTrainer, which will be used for training.\n",
        "\n",
        "tokenizer:\n",
        "\n",
        "The tokenizer that corresponds to the model. It is used for encoding and decoding inputs and outputs during training and evaluation.\n",
        "\n",
        "train_dataset=dataset:\n",
        "\n",
        "The training dataset to be used. This dataset will be used to train the model on specific tasks (e.g., SQL generation).\n",
        "\n",
        "dataset_text_field=\"text\":\n",
        "\n",
        "This indicates the field in the dataset that contains the input text. In this case, the dataset field is named text, which is where the formatted data (SQL queries, explanations, etc.) will be stored.\n",
        "\n",
        "max_seq_length=max_seq_length:\n",
        "\n",
        "The maximum sequence length that the model will process. Any input longer than this will be truncated.\n",
        "\n",
        "dataset_num_proc=2:\n",
        "\n",
        "Specifies the number of CPU processes used to process the dataset during training. Using more processes can speed up data processing, especially for large datasets.\n",
        "\n",
        "TrainingArguments (Key Hyperparameters):\n",
        "per_device_train_batch_size=2:\n",
        "\n",
        "Specifies the batch size per device. Since it's set to 2, this means that for each GPU (or CPU), 2 samples will be processed in parallel during each step.\n",
        "\n",
        "gradient_accumulation_steps=4:\n",
        "\n",
        "Gradient Accumulation allows for effective batch sizes larger than the device's physical memory limit. In this case, with 4 gradient accumulation steps, the model will accumulate gradients over 4 steps before updating the weights, which gives an effective batch size of 8 (2 per device * 4).\n",
        "\n",
        "warmup_steps=10:\n",
        "\n",
        "Warmup steps indicate how many steps the learning rate will gradually increase from 0 to the initial value during training. It helps in stabilizing the model's training early on.\n",
        "\n",
        "num_train_epochs=3:\n",
        "\n",
        "The number of epochs the model will be trained for. In this case, the model will train for 3 full passes through the dataset.\n",
        "\n",
        "learning_rate=2e-4:\n",
        "\n",
        "The learning rate is set to 0.0002. It controls the size of the steps the optimizer takes when updating the model's weights during training.\n",
        "\n",
        "fp16=not torch.cuda.is_bf16_supported() and bf16=torch.cuda.is_bf16_supported():\n",
        "\n",
        "These flags enable mixed precision training depending on whether bfloat16 (bf16) is supported on the device. This helps save memory and speeds up training.\n",
        "\n",
        "logging_steps=25:\n",
        "\n",
        "The logging frequency is set to 25 steps. This means that the training process will log information such as loss, learning rate, etc., every 25 steps.\n",
        "\n",
        "optim=\"adamw_8bit\":\n",
        "\n",
        "Specifies the optimizer to use during training. Here, the AdamW optimizer with 8-bit precision is used. 8-bit optimizers reduce the memory footprint, allowing for faster training with larger models.\n",
        "\n",
        "weight_decay=0.01:\n",
        "\n",
        "Weight decay helps regularize the model by penalizing large weights, which reduces overfitting.\n",
        "\n",
        "lr_scheduler_type=\"linear\":\n",
        "\n",
        "The learning rate scheduler determines how the learning rate changes during training. Here, it’s set to \"linear,\" meaning the learning rate will decrease linearly from its initial value to zero.\n",
        "\n",
        "seed=3407:\n",
        "\n",
        "The random seed is set to 3407 to ensure reproducibility of the results. Using the same seed, you will get the same random number generation, making the training process reproducible.\n",
        "\n",
        "output_dir=\"outputs\":\n",
        "\n",
        "The directory where the trained model will be saved at the end of training.\n",
        "\n",
        "save_strategy=\"epoch\":\n",
        "\n",
        "This defines how often the model will be saved. In this case, it will be saved at the end of every epoch.\n",
        "\n",
        "save_total_limit=2:\n",
        "\n",
        "The model will keep only the last 2 saved models. Older models will be automatically deleted to save disk space.\n",
        "\n",
        "dataloader_pin_memory=False:\n",
        "\n",
        "Disables pinning memory during data loading. Pinning memory can sometimes improve performance, but it's not necessary in all cases.\n",
        "\n",
        "report_to=\"none\":\n",
        "\n",
        "This disables reporting metrics and training logs to platforms like Weights & Biases. This is useful if you do not need such external integrations."
      ],
      "metadata": {
        "id": "bvRQsGIiUNZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lm8booC8XliQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4c177d7d8fd5431ba4347657cdfffde6",
            "9376a5a93c234ad397db918b9efb7991",
            "4fc3de17d26740469ee04aa0da68403e",
            "b30319b5c5b3412896ff584c73b46c17",
            "dec5ccbd08024647b766c0f1a80afe10",
            "43a40aa199ab4482a1ca5395943ae768",
            "a80eab1ea7a8474abe951eced175a1d5",
            "573a98f2e99545efb7cc4dd5a26740c6",
            "e1a2924fed7341748b8caa9187026e1c",
            "a0a55abb81654cb8aa22b06dd1671759",
            "333e5b8085494aa285f8763a53e57990"
          ]
        },
        "outputId": "ad849e1c-13aa-46a8-fbb7-7efcf072a39a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c177d7d8fd5431ba4347657cdfffde6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training arguments optimized for Unsloth\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=25,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        dataloader_pin_memory=False,\n",
        "        report_to=\"none\", # Disable Weights & Biases logging\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uZrtr0c4XmTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "452c0d81-590d-4478-9e8a-c3b18cd152b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 200 | Num Epochs = 3 | Total steps = 75\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 119,537,664 of 2,128,677,888 (5.62% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [75/75 06:19, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.552800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.282600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If using PEFT (LoRA adapter)\n",
        "from peft import PeftModel\n",
        "\n",
        "# Save and push\n",
        "model.save_pretrained(\"mymodel\")\n",
        "tokenizer.save_pretrained(\"mymodel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PgnVk9k1d4E",
        "outputId": "3141ed08-a7c3-49b8-db2a-245aea119c2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('mymodel/tokenizer_config.json',\n",
              " 'mymodel/special_tokens_map.json',\n",
              " 'mymodel/chat_template.jinja',\n",
              " 'mymodel/tokenizer.model',\n",
              " 'mymodel/added_tokens.json',\n",
              " 'mymodel/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "qoBj-75TadOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Enable Unsloth 2x faster inference\n",
        "from unsloth import FastLanguageModel\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# ---- 1. Build prompt just like during training ----\n",
        "def build_prompt(company_db, sql_prompt):\n",
        "    return f\"\"\"Below is an instruction that describes a task, paired with an input and expected output.\n",
        "### Instruction:\n",
        "Company database: {company_db}\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: {sql_prompt}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "# ---- 2. Define your question ----\n",
        "company_database = \"CREATE TABLE employees (id INT, name TEXT, salary REAL, department TEXT);\"\n",
        "question = \"What is the average salary per department?\"\n",
        "\n",
        "# ---- 3. Format the full prompt ----\n",
        "prompt = build_prompt(company_database, question)\n",
        "\n",
        "# ---- 4. Tokenize input prompt ----\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# ---- 5. Generate response ----\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "# ---- 6. Decode and clean output ----\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "response_only = decoded_output.split(\"### Response:\")[-1].split(\"<|endoftext|>\")[0].strip()\n",
        "\n",
        "# ---- 7. Print the final result ----\n",
        "print(\"=== Model Response ===\\n\", response_only)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuYz8trGzQ6t",
        "outputId": "787f39e3-e354-4f9d-9e6f-de1f92cbdbd3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model Response ===\n",
            " SQL: SELECT department, AVG(salary) FROM employees GROUP BY department;\n",
            "Explanation: The SQL query calculates the average salary for each department by grouping the records by the department column and applying the AVG function to the salary column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(db_schema, sql_prompt):\n",
        "    return f\"\"\"Below is an instruction that describes a task, paired with an input and expected output.\n",
        "### Instruction:\n",
        "Company database: {db_schema}\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: {sql_prompt}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "test_cases = [\n",
        "    (\"CREATE TABLE employees (id INT, name TEXT, salary REAL);\", \"What is the total number of employees?\"),\n",
        "    (\"CREATE TABLE employees (id INT, name TEXT, salary REAL);\", \"Get the names of employees who earn more than 50000.\"),\n",
        "    (\"CREATE TABLE employees (id INT, name TEXT, salary REAL, department TEXT);\", \"What is the average salary per department?\"),\n",
        "    (\"CREATE TABLE employees (id INT, name TEXT, salary REAL);\", \"Who is the highest-paid employee?\"),\n",
        "    (\"CREATE TABLE employees (id INT, name TEXT, department TEXT);\", \"List departments that have more than 5 employees.\"),\n",
        "]\n",
        "\n",
        "for db_schema, prompt in test_cases:\n",
        "    full_prompt = build_prompt(db_schema, prompt)\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"❓ Prompt: {prompt}\\n📄 Response:\\n{response.split('### Response:')[-1].split('<|endoftext|>')[0].strip()}\\n{'-'*80}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka65fi5YDaHB",
        "outputId": "69088104-88a7-4f11-9c03-e5f9e8e8552e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❓ Prompt: What is the total number of employees?\n",
            "📄 Response:\n",
            "SQL: SELECT COUNT(*) FROM employees;\n",
            "Explanation: This SQL query calculates the total number of employees by using the COUNT function to count the number of rows in the employees table.\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: Get the names of employees who earn more than 50000.\n",
            "📄 Response:\n",
            "SQL: SELECT name FROM employees WHERE salary > 50000;\n",
            "Explanation: This query retrieves the names of employees who earn more than 50000 by selecting the name column from the employees table where the salary is greater than 50000.\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: What is the average salary per department?\n",
            "📄 Response:\n",
            "SQL: SELECT department, AVG(salary) FROM employees GROUP BY department;\n",
            "Explanation: This SQL query calculates the average salary per department. It uses the AVG() function to calculate the average value of the salary column, and the GROUP BY clause to group the results by department.\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: Who is the highest-paid employee?\n",
            "📄 Response:\n",
            "SQL: SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\n",
            "Explanation: This query orders the employees table by salary in descending order and returns the name of the highest-paid employee.\n",
            "--------------------------------------------------------------------------------\n",
            "❓ Prompt: List departments that have more than 5 employees.\n",
            "📄 Response:\n",
            "SQL: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 5;\n",
            "Explanation: The SQL query lists the departments that have more than 5 employees by grouping the 'employees' table by the 'department' column and filtering the groups using the HAVING clause.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fine-tuned model\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Test prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Extract the product information:\\n<div class='product'><h2>iPad Air</h2><span class='price'>$1344</span><span class='category'>audio</span><span class='brand'>Dell</span></div>\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.batch_decode(outputs)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "id": "M-S8T8wq1yYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twEmkIrLZLtD"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSmERd43la0l"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "if gguf_files:\n",
        "    gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
        "    print(f\"Downloading: {gguf_file}\")\n",
        "    files.download(gguf_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iV1-_-UGpYA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsloth: A library that provides fast and efficient models, including features for working with PEFT (Parameter Efficient Fine-Tuning) adapters, and the ability to load models optimized for low-precision computation.\n",
        "\n",
        "torch: PyTorch is a deep learning framework, and it’s imported to handle tensor operations and work with GPU acceleration.\n",
        "\n",
        "max_seq_length: Defines the maximum sequence length (2048 tokens). This parameter ensures that the input sequences don’t exceed this length when being tokenized.\n",
        "\n",
        "dtype: Specifies the data type used for the model’s computations (e.g., float16, float32). Setting it to None means it will auto-detect the most appropriate type based on the hardware.\n",
        "\n",
        "load_in_4bit=True: This option loads the model in 4-bit precision, optimizing memory usage and improving performance, especially useful for large models that may otherwise be too memory-intensive for your GPU."
      ],
      "metadata": {
        "id": "Pk9W6ZF5juNT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j34K2RoipYPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAS1iK6BpYSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cho0l3hipYVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lA3L7UfzpYYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUdez-EkpYbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vs9ZsRVLpYeS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c177d7d8fd5431ba4347657cdfffde6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9376a5a93c234ad397db918b9efb7991",
              "IPY_MODEL_4fc3de17d26740469ee04aa0da68403e",
              "IPY_MODEL_b30319b5c5b3412896ff584c73b46c17"
            ],
            "layout": "IPY_MODEL_dec5ccbd08024647b766c0f1a80afe10"
          }
        },
        "9376a5a93c234ad397db918b9efb7991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43a40aa199ab4482a1ca5395943ae768",
            "placeholder": "​",
            "style": "IPY_MODEL_a80eab1ea7a8474abe951eced175a1d5",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "4fc3de17d26740469ee04aa0da68403e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_573a98f2e99545efb7cc4dd5a26740c6",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1a2924fed7341748b8caa9187026e1c",
            "value": 200
          }
        },
        "b30319b5c5b3412896ff584c73b46c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0a55abb81654cb8aa22b06dd1671759",
            "placeholder": "​",
            "style": "IPY_MODEL_333e5b8085494aa285f8763a53e57990",
            "value": " 200/200 [00:00&lt;00:00, 1832.88 examples/s]"
          }
        },
        "dec5ccbd08024647b766c0f1a80afe10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a40aa199ab4482a1ca5395943ae768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a80eab1ea7a8474abe951eced175a1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "573a98f2e99545efb7cc4dd5a26740c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a2924fed7341748b8caa9187026e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0a55abb81654cb8aa22b06dd1671759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "333e5b8085494aa285f8763a53e57990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}